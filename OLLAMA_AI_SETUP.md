# üß† Configuraci√≥n AI Local con Ollama

## üéØ ¬øQu√© es Ollama?

**Ollama** es la mejor plataforma para ejecutar modelos de IA de √∫ltima generaci√≥n **completamente gratis y en tu computadora local**.

### ‚úÖ Ventajas
- **100% Gratis**: Sin l√≠mites de uso, sin API keys
- **100% Privado**: Tus datos nunca salen de tu PC
- **Sin Internet**: Funciona offline despu√©s de descargar el modelo
- **Modelos Potentes**: llama3.2, mistral, codellama, phi3, qwen2.5, deepseek, gemma2
- **Aprendizaje Continuo**: Sistema de cach√© inteligente que mejora con el tiempo

---

## üöÄ Instalaci√≥n R√°pida (3 pasos)

### 1Ô∏è‚É£ Descarga e Instala Ollama

**Windows:**
```bash
# Descarga desde: https://ollama.com/download
# Ejecuta el instalador y sigue las instrucciones
```

**macOS:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 2Ô∏è‚É£ Inicia el Servidor

```bash
# Ejecuta en terminal (se queda corriendo en background)
ollama serve
```

### 3Ô∏è‚É£ Descarga un Modelo Potente

```bash
# Opci√≥n 1: Llama 3.2 - R√°pido y eficiente (RECOMENDADO)
ollama pull llama3.2

# Opci√≥n 2: Mistral - Muy inteligente
ollama pull mistral

# Opci√≥n 3: Qwen 2.5 - Excelente en espa√±ol
ollama pull qwen2.5

# Opci√≥n 4: CodeLlama - Especializado en c√≥digo
ollama pull codellama

# Opci√≥n 5: Phi 3 - Ultra r√°pido de Microsoft
ollama pull phi3
```

---

## üé® Uso en la Aplicaci√≥n

### Configurar en FlowDistributor

1. **Abre el AI Assistant** (√≠cono de cerebro üß† flotante)
2. **Haz clic en ‚öôÔ∏è** (bot√≥n de configuraci√≥n)
3. **Selecciona tu modelo** en el dropdown
4. **Guarda la configuraci√≥n**

### Modelos Disponibles

| Modelo | Tama√±o | Velocidad | Inteligencia | Uso Recomendado |
|--------|--------|-----------|--------------|-----------------|
| üöÄ **llama3.2** | 3B | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | General (RECOMENDADO) |
| üß† **mistral** | 7B | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Tareas complejas |
| üíª **codellama** | 7B | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Programaci√≥n |
| ‚ö° **phi3** | 3.8B | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Respuestas r√°pidas |
| üåç **qwen2.5** | 7B | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Multiling√ºe/Espa√±ol |
| üîß **deepseek-coder** | 33B | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | C√≥digo avanzado |
| üíé **gemma2** | 9B | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Google, muy bueno |

---

## üîß Comandos √ötiles

```bash
# Ver modelos instalados
ollama list

# Eliminar un modelo
ollama rm llama3.2

# Actualizar un modelo
ollama pull llama3.2

# Ver informaci√≥n del modelo
ollama show llama3.2

# Probar un modelo en terminal
ollama run llama3.2

# Ver logs
ollama logs
```

---

## üéØ Sistema de Aprendizaje Continuo

### C√≥mo Funciona

1. **Cach√© Inteligente**: Guarda las √∫ltimas 200 conversaciones
2. **Fallback Inteligente**: Si Ollama no est√° disponible, usa respuestas previas similares
3. **Mejora Continua**: Aprende patrones de tus preguntas para mejorar respuestas

### Datos Guardados (localStorage)

```javascript
// Ver datos de aprendizaje en consola del navegador
JSON.parse(localStorage.getItem('ai_learning_data'))

// Limpiar cach√© si es necesario
localStorage.removeItem('ai_learning_data')
```

---

## üêõ Soluci√≥n de Problemas

### ‚ùå Error: "No se pudo conectar con Ollama"

**Soluci√≥n 1:** Verifica que Ollama est√© corriendo
```bash
ollama serve
```

**Soluci√≥n 2:** Verifica el host en configuraci√≥n
- Por defecto: `http://localhost:11434`
- Cambia en ‚öôÔ∏è si usas puerto diferente

### ‚ùå Error: "Modelo no encontrado"

**Soluci√≥n:** Descarga el modelo
```bash
ollama pull llama3.2
```

### ‚ùå Respuestas lentas

**Soluci√≥n 1:** Usa un modelo m√°s peque√±o (llama3.2 o phi3)
**Soluci√≥n 2:** Cierra aplicaciones pesadas
**Soluci√≥n 3:** Aumenta RAM del modelo en Ollama config

---

## üìä Requisitos del Sistema

### M√≠nimos
- **RAM**: 8 GB (para modelos 3B)
- **Almacenamiento**: 5 GB por modelo
- **CPU**: Cualquier procesador moderno

### Recomendados
- **RAM**: 16 GB+ (para modelos 7B+)
- **GPU**: NVIDIA/AMD (opcional, acelera respuestas)
- **Almacenamiento**: SSD (mejora velocidad de carga)

---

## üéì Mejores Pr√°cticas

### Para Mejor Rendimiento
1. **Usa llama3.2** si quieres velocidad + calidad
2. **Usa mistral** si necesitas m√°xima inteligencia
3. **Mant√©n Ollama siempre corriendo** en background
4. **No cambies de modelo** constantemente (carga en RAM)

### Para Mejor Privacidad
1. ‚úÖ Todo es local, nada se env√≠a a servidores
2. ‚úÖ No se requiere internet despu√©s de descargar
3. ‚úÖ Los datos de aprendizaje est√°n en tu navegador (localStorage)

---

## üåü Pr√≥ximas Mejoras

- [ ] Streaming de respuestas (ver texto mientras se genera)
- [ ] Modo multi-modelo (combina varios modelos)
- [ ] Fine-tuning personalizado por sistema
- [ ] Integraci√≥n con archivos adjuntos
- [ ] Exportar/importar cach√© de aprendizaje

---

## üìö Recursos

- **Sitio Oficial**: https://ollama.com
- **Documentaci√≥n**: https://github.com/ollama/ollama/blob/main/docs/api.md
- **Modelos Disponibles**: https://ollama.com/library
- **Comunidad**: https://discord.gg/ollama

---

## ‚úÖ Checklist de Configuraci√≥n

- [ ] Ollama instalado
- [ ] `ollama serve` corriendo
- [ ] Modelo descargado (`ollama pull llama3.2`)
- [ ] Configuraci√≥n guardada en AI Assistant (‚öôÔ∏è)
- [ ] Primera pregunta de prueba realizada
- [ ] Sistema funcionando correctamente

---

**¬°Listo!** Ahora tienes un asistente IA de √∫ltima generaci√≥n corriendo 100% gratis y privado en tu computadora üöÄ
